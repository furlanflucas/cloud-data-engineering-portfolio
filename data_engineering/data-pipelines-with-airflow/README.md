# **Airflow & Redshift Data Pipeline**

## **Project Overview**
This project demonstrates an **ETL pipeline using Apache Airflow and AWS Redshift**, designed to process and transform log data from S3 into a structured analytical format. The pipeline follows a **DAG (Directed Acyclic Graph) architecture**, orchestrating data extraction, loading, transformation, and validation.

### **Key Features**
- **Data Ingestion**: Extracts raw log data from **AWS S3** and stages it into **Amazon Redshift**.
- **Data Transformation**: Loads staging data into **fact and dimension tables**.
- **Automated Workflow**: Uses **Apache Airflow** to manage task dependencies and scheduling.
- **Data Quality Checks**: Ensures integrity using SQL-based **data validation tests**.
- **Scalability**: Supports large-scale **log data processing** in the cloud.

---

## **Technologies Used**
- **Apache Airflow** â€“ DAG orchestration
- **Amazon Redshift** â€“ Data warehouse
- **Amazon S3** â€“ Storage for raw log files
- **Python** â€“ ETL scripting and SQL execution
- **SQL** â€“ Data transformations and integrity checks
- **AWS IAM** â€“ Credential management

---

## **Project Structure**
```
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ udac_dag.py  # Main Airflow DAG definition
â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ helpers/
â”‚   â”‚   â”œâ”€â”€ sql_queries.py  # SQL queries for transformations
â”‚   â”œâ”€â”€ operators/
â”‚   â”‚   â”œâ”€â”€ stage_redshift.py  # Loads data from S3 to Redshift
â”‚   â”‚   â”œâ”€â”€ load_fact.py  # Loads data into fact table
â”‚   â”‚   â”œâ”€â”€ load_dimension.py  # Loads data into dimension tables
â”‚   â”‚   â”œâ”€â”€ data_quality.py  # Performs data validation
â”œâ”€â”€ README.md  # Project documentation
```

---

## **ETL Pipeline Workflow**

1. **Stage Events & Songs Data from S3 to Redshift**
   - Reads raw log and song data from **S3**.
   - Loads it into Redshift **staging tables**.

2. **Load Data into Fact Table**
   - Transforms staged data and inserts records into the **songplays fact table**.

3. **Load Data into Dimension Tables**
   - Populates **users, songs, artists, and time** dimension tables.
   - Uses `truncate-before-insert` strategy for idempotency.

4. **Run Data Quality Checks**
   - Ensures **no NULL values in primary keys**.
   - Confirms **row counts match expected values**.

---

## **Installation & Setup**
### **1. Clone the Repository**
```sh
git clone https://github.com/yourusername/airflow-redshift-etl.git
cd airflow-redshift-etl
```

### **2. Install Dependencies**
```sh
pip install apache-airflow
```

### **3. Configure Airflow & AWS**
- Set up **AWS credentials** in Airflow connections (`aws_credentials`).
- Configure **Redshift connection (`redshift`)** in Airflow.

### **4. Start Airflow Scheduler & Webserver**
```sh
airflow scheduler & airflow webserver -p 8080
```
Access the Airflow UI at: **`http://localhost:8080`**

### **5. Trigger the DAG**
In Airflow UI, **turn on** the `udac_dag` DAG and trigger execution.

---

## **SQL Queries Used**
### **Fact Table (songplays)**
```sql
SELECT
    md5(events.sessionid || events.start_time) AS songplay_id,
    events.start_time,
    events.userid,
    events.level,
    songs.song_id,
    songs.artist_id,
    events.sessionid,
    events.location,
    events.useragent
FROM staging_events events
LEFT JOIN staging_songs songs
ON events.song = songs.title
AND events.artist = songs.artist_name;
```

### **Dimension Tables**
```sql
SELECT DISTINCT userid, firstname, lastname, gender, level
FROM staging_events
WHERE page='NextSong';
```

---

## **Future Enhancements**
âœ… **Deploy on AWS MWAA (Managed Workflows for Apache Airflow)**
âœ… **Integrate Airflow with AWS Lambda for event-driven processing**
âœ… **Implement partitioning for Redshift tables for better query performance**
âœ… **Add automated alerting via Slack or AWS SNS**

---

## **Contributions & Contact**
ðŸ”¹ If you'd like to contribute, feel free to submit a **pull request**.
ðŸ”¹ Questions? Reach out via GitHub issues or connect with me!